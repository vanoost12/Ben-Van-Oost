# -*- coding: utf-8 -*-
"""Hw1_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ha53qpSBVpfHJpXM0Pq5GzmMJWCLfh83
"""

"""
1- section 1
"""
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

sigma = 1
size = 700
# Creating 700 Guasian variable

train_set = pd.DataFrame(columns=['x', 'y', 'label'])
for i in range(size + 1):
    x = random.randint(1, 3)
    if (x == 1):
        s = np.random.normal(loc=[-1, 1], scale=1)
        train_set.loc[i] = [s[0], s[1], "mu " + str(x)]
    if (x == 2):
        s = np.random.normal(loc=[-2.5, 2.5], scale=1)
        train_set.loc[i] = [s[0], s[1], "mu " + str(x)]

    if (x == 3):
        s = np.random.normal(loc=[-4.5, 4.5], scale=1)
        train_set.loc[i] = [s[0], s[1], "mu " + str(x)]

'''''''''
1 - section 2
Plot the train set
'''''''''
colors = ['g', 'b', 'r']
labels = train_set['label'].unique().tolist()

def plot_Gaus_scatter_plot(train_set, x_column='x', y_columm='y'):
    for label, color in zip(labels, colors):
        plot_data = train_set[train_set['label'] == label]
        plt.scatter(x=plot_data[x_column], y=plot_data[y_columm], s=10, label=label, c=color, data=None)
    plt.title(f'Gaussian - {x_column} vs. {y_columm} Train set')
    plt.xlabel(x_column)
    plt.ylabel(y_columm)
    plt.legend()
    plt.grid(True)


plot_Gaus_scatter_plot(train_set)

'''''''''
1 - section 3
Generate 300 additional samples
'''''''''
test_set = pd.DataFrame(columns=['x', 'y', 'label'])
test_size = 300
for i in range(test_size + 1):
    x = random.randint(1, 3)
    if (x == 1):
        s = np.random.normal(loc=[-1, 1], scale=1)
        test_set.loc[i] = [s[0], s[1], "mu " + str(x)]
    if (x == 2):
        s = np.random.normal(loc=[-2.5, 2.5], scale=1)
        test_set.loc[i] = [s[0], s[1], "mu " + str(x)]

    if (x == 3):
        s = np.random.normal(loc=[-4.5, 4.5], scale=1)
        test_set.loc[i] = [s[0], s[1], "mu " + str(x)]

def plot_Gaus_scatter_plot(train_set, x_column='x', y_columm='y'):
    for label, color in zip(labels, colors):
        plot_data = train_set[train_set['label'] == label]
        plt.scatter(x=plot_data[x_column], y=plot_data[y_columm], s=10, label=label, c=color, data=None)
    plt.title(f'Gaussian - {x_column} vs. {y_columm} Test set')
    plt.legend()
    plt.xlabel(x_column)
    plt.ylabel(y_columm)
    plt.grid(True)


plot_Gaus_scatter_plot(test_set)

"""
1 -section 4
 Train a k-NN classifier using k=1 with Euclidean distance on the 700 samples (train set) and evaluate
it on the 300 examples (test set).
"""
from sklearn.neighbors import KNeighborsClassifier

# Define k
k = 1
# Define training columns
x_cols = ['x', 'y']
y_cols = "label"
# Instantiate the classifier
model = KNeighborsClassifier(n_neighbors=k)
# Fit on the training set
model.fit(X=train_set[x_cols], y=train_set[y_cols])

y_train_pred = model.predict(X=train_set[x_cols])
y_test_pred = model.predict(X=test_set[x_cols])
#Calculate the test error as defined
y_test_error = (1/test_size)*sum(y_test_pred != test_set['label'])
#Calculate the train error as defined
y_train_error = (1/size)*sum(y_train_pred != train_set['label'])
print("test error", y_test_error)
print("Train error" , y_train_error)

"""
test error 0.20333333333333334
Train error 0.0
We can notice that there is a gap between the errors.
The gap occur as a result of k value. On the train set, when setting k to 1 we compute distances from each point to the nearset neighbor aka 
itself therefore the distance of each point from itself is zero. On the other hand we can notice that the error is bigger on the 
test set because the nearest point is measured from the train set, meaning its not the point it self. This phenomenon is known as overfitting, outliers can 
strongly effect on the prediction of the model.

"""

'''''''''
1 - section 5
'''''''''
from sklearn.neighbors import KNeighborsClassifier
test_error= []
train_error = []
for k in range(1,21):
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X=train_set[x_cols], y=train_set[y_cols])
    y_train_predict = model.predict(X=train_set[x_cols])
    y_test_predict = model.predict(X=test_set[x_cols])
    y_train_true = train_set[y_cols].values
    y_test_true = test_set[y_cols].values
    test_error.append((1/test_size)*sum(y_test_predict != y_test_true))
    train_error.append((1/size)*sum(y_train_predict != y_train_true))
  

plt.plot(np.arange(1,21),train_error,label = "train_set")
plt.plot(np.arange(1,21),test_error,label ="test_set")
plt.xlabel("k")
plt.ylabel("ErrorRate")
plt.grid(True)
plt.legend()

"""As we can see the error rate on the test set decrese as k increse until it converges to a certain value . 
It wont happend all the time, counter example:
Let the size of the train sample  be 5.Let the number of 'blue' labeled samples in the train set be 3 and let the number of 'red' labeled samples be 2,let k be 5.
Let the test set be 5 red points, in this case each point will be labeled as Blue. Will get 0% accuracy  and 100% error set because all the points 
will be classfied as Blue even though they are red.
In genral,let k be the size of the train set.In this case each point will be classified as the label that is most common in the train set.
Therefore increasing k can decrese the error rate to a certain point but increasing k constantly can cause overfitting and increase the error rate.

"""

""""
1- section 6
"""
from sklearn.neighbors import KNeighborsClassifier
k = 10
test100_set = pd.DataFrame(columns=['x', 'y', 'label'])
for i in range(101):
   x = random.randint(1, 3)
   if (x == 1):
    s = np.random.normal(loc=[-1, 1], scale=1)
    test100_set.loc[i] = [s[0], s[1], str(x)]
   if (x == 2):
    s = np.random.normal(loc=[-2.5, 2.5], scale=1)
    test100_set.loc[i] = [s[0], s[1], str(x)]
   if (x == 3):
    s = np.random.normal(loc=[-4.5, 4.5], scale=1)
    test100_set.loc[i] = [s[0], s[1], str(x)]
test_errors = []
train_errors = []
#Genratin 10,15...40 samples
for  m_train in range(10,45,5):
  train_m_set = pd.DataFrame(columns=['x', 'y', 'label'])
  for i in range(m_train):
    x = random.randint(1, 3)
    if (x == 1):
      s = np.random.normal(loc=[-1, 1], scale=1)
      train_m_set.loc[i] = [s[0], s[1], str(x)]
    if (x == 2):
      s = np.random.normal(loc=[-2.5, 2.5], scale=1)
      train_m_set.loc[i] = [s[0], s[1], str(x)]
    if (x == 3):
      s = np.random.normal(loc=[-4.5, 4.5], scale=1)
      train_m_set.loc[i] = [s[0], s[1], str(x)]
  # Define training columns
  x_cols = ['x', 'y']
  y_cols = "label"
  # Instantiate the classifier
  model = KNeighborsClassifier(n_neighbors = k)
  # Fit on the training set
  model.fit(X=train_m_set[x_cols], y=train_m_set[y_cols])
  y_test_pred = model.predict(X=test100_set[x_cols])
  y_train_pred = model.predict(X=train_m_set[x_cols])
  #Calculate the test error as defined
  y_test_error = (1/100)*sum(y_test_pred != test100_set['label'])
  #Calculate the train error as defined
  y_train_error = (1/m_train)*sum(y_train_pred != train_m_set['label'])
  train_errors.append(y_train_error)
  test_errors.append(y_test_error)

plt.plot(np.arange(10,45,5),train_errors,label = "train_set")
plt.plot(np.arange(10,45,5),test_errors,label ="test_set")
plt.xlabel("M test- train set size")
plt.ylabel("ErrorRate")
plt.grid(True)
plt.legend()
plt.show()

"""
We will expact that the train and test error rate will decrese as a function of train set size. 
With more data the training set will be more robust to outliers and will represent the real distribution better.
The training set error rate will decrease as well because outliers will efect less on the error rate, each point
will have less weight.
As we can see our exception match the graph behavior."""



"""1-section 7

Repeating step 6, 10 trials changes each time the lines as function of train set size.
In all the trials we can notice that there is a clear downward trend as function of the train set size.
The fact that the training set differ each time and because it's size is small, every point has more influence on the error rate.
The small size of the training set makes its more sensitive to outliers.
However all of the trials indicate a decrease of the train and test error as a function of the train set size.
(match our exception from section 6)
"""



"""
1 - section 8

We will define the weight "function" by calculating the distance between test point x and neighbor y by : 1/distance(x,y),this method will give
bigger weight to close distances and lower weight to big distances.

* For every point x pick the k nearset neighboors.
* For every point calculate the inverse distance to it's k neighboor aka 1/distance.
* For eace label sum the inverse distances.
* Classify each point to the max(label_1_inverse_distances,....,label_n_inverse_distances) label.

"""